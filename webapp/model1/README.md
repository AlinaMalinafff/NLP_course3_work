---
tags:
- setfit
- sentence-transformers
- text-classification
- generated_from_setfit_trainer
widget:
- text: Я бы сделала карточки, вот как в Квизлите. То есть перевод на русском и английское
    слово. И вот так. Возможно, ну, с картинкой какой-то ассоциации, но это уже надо
    тоже придумывать. А это долго. Вряд ли я буду этим заниматься.
- text: Ну, это, безусловно зависит от позиции кандидата, потому что есть люди, у
    которых есть опыт, условно, в нашей сфере, то есть они, например, могли успеть
    поработать где-нибудь в корпоративной стратегии, в какой-нибудь банке или в какой-нибудь
    иной компании, в ВК, Яндекс, Авито, в любой большой такой,  плюс-минус устоявшейся
    компании, вот. Например, кандидатам с таким опытом, там есть эти условия, в том
    плане, что предполагается, что они быстро будут увлекаться, смогут быстро уловить
    суть системы, структуры взаимоотношений, которые есть, смогут брать на себя определенную
    ответственность, действовать автономно в некотором роде, вот, например от сотрудников
    помладше с очень малым, ну или, не знаю, пару лет опыта, например, если есть за
    душой, но, например, не совсем релевантным для нас, как бы сотрудник должен быть
    проактивен, он должен иметь хороший аналитический скилл, ну и в целом, конечно,
    иметь какое-то понимание, как устроен рынок. Соответственно, ожидания от него,
    понятно, что любой сотрудник имеет испытательный срок, 3 месяца всего лишь, нельзя
    рассчитывать на то, что недостаточно опытный сотрудник за 3 месяца станет полноценным
    членом команды, в том смысле, что сможет взаимодействовать автономно, понимать
    все, как устроены процессы, сможет выполнять любые задачи. Конечно, нет, как практика
    показывает, нужно как минимум пару лет опыта, чтобы в принципе пройти. В стратегии
    есть своя специфика, есть стратегический цикл. Он предполагает то, что, например,
    к концу года подводятся итоги по стратегии. Соответственно, это, по сути, одна
    из функций мониторинга, когда формируются статусы, в принципе, общая картинка,
    и выносится вместе со статусами предложение о пересмотре или по актуализация каких-то
    положений. Например, условно, если сотрудник пришел весной, за три месяца он,
    конечно, не дойдет до этого этапа, потому что он происходит в конце года. Для
    того, чтобы сотрудник в нашей сфере получил весь необходимый опыт, как минимум
    два стратегических цикло должно пройти, то есть два года.
- text: 'Мне нравится работать в команде, потому что я считаю, что слаженная работа
    и обмен идеями — это ключ к успеху. Например, на одном из проектов я тесно сотрудничал
    с коллегами из спецотдела маркетинга, чтобы построить модель прогнозирования поведения
    клиентов на основе их 80-х действий. Это было сложно, потому что потребности отдела
    маркетинга не всегда можно было легко перевести в числовые данные, но благодаря
    постоянным обсуждениям и обмену мнениями мы нашли оптимальный подход.

    В процессе работы я помогал журналист с технической стороны, объясняя, как можно
    использовать наши данные для более точных прогнозов. В итоге, мы создали модель,
    которая значительно понижать эффективность рекламных кампаний и приноситься компании
    значительную экономию.'
- text: Это, да. Я приехала после... Нет, не после чего. Ну ладно, из дома, но...
    Короче, поднялась температура. Во время работы. Ну, не сильно, потом я выпила
    вроде чай, всё было нормально, потом она опустилась. А потом уже, когда возвращалась
    домой, она опять поднялась. Вот, ну, короче, с такой полутемпературой был этот
    день. И ты вот это вот стоишь, ещё один закрываешься, тебя вот так. То жарко,
    то холод, и ты вроде думаешь, ну, нормально, я стою, вроде не падаю. Но всё равно
    сложно.
- text: Успех однозначно, в любом случае, он есть при любых переговорах, даже негативных.
    Это тоже результат, это опыт. Однозначно. И если с таким подходам - как раз это
    помогает решению всех задач… и не зацикливаться, что там ничего не получается.
    Это определенный этап. С точки зрения быстрого успеха это, конечно, индикатор.
    Я рассматриваю, как оценка в школе. Она менее важна, важны знания. Но индикатор
    твоих знаний, это как раз оценка. И многие иногда путают, в том числе и мои дети,
    что оценка там важна. Нет, нам не нужна оценка. Просто это индикатор насколько
    ты эффективно это сделал. И самое главное – коммуникация. То же самое и в бизнесе.
    Наверное, индикатор – это деньги. Но это не то, что мы гонимся за деньгами. Это
    просто показатель того, смог ли ты сделать что-то правильно или не смог. То есть
    насколько нужна твоя деятельность. Но, опять-таки, это в бизнесе. Если касается
    ассоциации, то, опять-таки, наверное, это больше оборот общий компаний. Но в итоге
    – да. Но на каждом шаге она не обязательно может показаться в деньгах. Она может
    показаться на контактах, на перспективах. Ну и, конечно, итоговый результат экономики,
    чем мы занимаемся – это… индикатор - это показатель средств, которые прошли или
    получили доход. Причем не обязательно, что доход получил я, а доход получил, допустим,
    определенный ряд компаний или страна или государство. Это тоже показатель успешных
    переговоров.
metrics:
- accuracy
pipeline_tag: text-classification
library_name: setfit
inference: true
base_model: sentence-transformers/paraphrase-MiniLM-L3-v2
---

# SetFit with sentence-transformers/paraphrase-MiniLM-L3-v2

This is a [SetFit](https://github.com/huggingface/setfit) model that can be used for Text Classification. This SetFit model uses [sentence-transformers/paraphrase-MiniLM-L3-v2](https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L3-v2) as the Sentence Transformer embedding model. A [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) instance is used for classification.

The model has been trained using an efficient few-shot learning technique that involves:

1. Fine-tuning a [Sentence Transformer](https://www.sbert.net) with contrastive learning.
2. Training a classification head with features from the fine-tuned Sentence Transformer.

## Model Details

### Model Description
- **Model Type:** SetFit
- **Sentence Transformer body:** [sentence-transformers/paraphrase-MiniLM-L3-v2](https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L3-v2)
- **Classification head:** a [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) instance
- **Maximum Sequence Length:** 128 tokens
- **Number of Classes:** 3 classes
<!-- - **Training Dataset:** [Unknown](https://huggingface.co/datasets/unknown) -->
<!-- - **Language:** Unknown -->
<!-- - **License:** Unknown -->

### Model Sources

- **Repository:** [SetFit on GitHub](https://github.com/huggingface/setfit)
- **Paper:** [Efficient Few-Shot Learning Without Prompts](https://arxiv.org/abs/2209.11055)
- **Blogpost:** [SetFit: Efficient Few-Shot Learning Without Prompts](https://huggingface.co/blog/setfit)

### Model Labels
| Label       | Examples                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
|:------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Процесс     | <ul><li>'Ну да, пораньше пойти домой, это тоже, это считай как мотивация денежная, то есть тебе предложили там в два раза больше. Уйти пораньше с работы, ещё если каждый день, то да. Ну и, соответственно, да, если зарплата, если там какие-то дополнительные выходные, дополнительный отпуск оплачиваемый. Желательно. Да, вот такие вот штуки нравятся. Потому что кому-то там достаточно просто мотивашки какой-то сказать. Вот, ты молодец, мы с тобой справимся. Ну, не. Это не моё. Я всё-таки привыкла, что ты что-то делаешь, ты должен что-то получать, соответственно. Слова мне любой человек может сказать. А условно-дополнительное выходной или поднятие зарплаты, ну, нет. Не любой человек может.'</li><li>'Да, да, каждый раз там могли, ну, тысячу, я думаю, ну ладно, тысяча. Ну потом две, ну как бы ладно. А потом, ну и у меня зарплата тогда вышла, то есть условно 20 выходило, там тысячу снимали, я такая, ну ладно, ладно. Незаметно как будто бы. А когда у тебя 8 тысяч, вот у тебя половину просто забрали, ты такой, обидно.'</li><li>'Ну да, у нас камера со гудение. Ну поэтому каждый раз, да, вот такой думаешь, блин, человекомя лгать везде.'</li></ul>                                                                                                    |
| Результат   | <ul><li>'Моя должность была бариста-кассир в кофейне, в сети кофеен. Я занималась, собственно пробиванием товаров на кассе, готовка кофе, всех, в принципе, напитков, которые были представлены в меню по технической карте, уборка помещения полностью перед закрытием и закрытие кафе.'</li><li>'Мы сделали ставку на инфлюенсеров в TikTok, рассчитывая быстро нарастить трафик среди молодёжной аудитории. Потратили больше $20\u202f000, а полученный эффект оказался незначительным — трафик был, но не конвертировался.\nЯ быстро свернул кампанию, провёл аудит и перераспределил средства в контекстную рекламу и SEO, что дало стабильный рост органики. Для меня важно не застревать в ошибках — если что-то не даёт результата, я действую жёстко и быстро.'</li><li>'Да, конечно. Например, в одном проекте мы изначально заложили избыточно сложную систему согласований с юридическим отделом покупателя, и это замедляло реализацию почти на 30 %. Я пересмотрел цепочку — ввёл шаблоны, уточнять границы ответственности и добиваться делегирования ряда решений менеджеру на стороне клиента.\nВ итоге сроки нормализовались, и мы даже сократили общее время реализации на 10 % по сравнению с исходным планом. Иногда нужно усложнить ход, чтобы не пострадал итог.'</li></ul> |
| Нейтральный | <ul><li>'Ну, именно качество или тоже какие-то навыки?'</li><li>'Именно на той работе?'</li><li>'Изначально побудило уйти в это направление желание развиваться в финансовой практике.'</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |

## Uses

### Direct Use for Inference

First install the SetFit library:

```bash
pip install setfit
```

Then you can load this model and run inference.

```python
from setfit import SetFitModel

# Download from the 🤗 Hub
model = SetFitModel.from_pretrained("setfit_model_id")
# Run inference
preds = model("Я бы сделала карточки, вот как в Квизлите. То есть перевод на русском и английское слово. И вот так. Возможно, ну, с картинкой какой-то ассоциации, но это уже надо тоже придумывать. А это долго. Вряд ли я буду этим заниматься.")
```

<!--
### Downstream Use

*List how someone could finetune this model on their own dataset.*
-->

<!--
### Out-of-Scope Use

*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->

<!--
## Bias, Risks and Limitations

*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->

<!--
### Recommendations

*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->

## Training Details

### Training Set Metrics
| Training set | Min | Median  | Max |
|:-------------|:----|:--------|:----|
| Word count   | 3   | 82.6429 | 628 |

| Label       | Training Sample Count |
|:------------|:----------------------|
| Нейтральный | 7                     |
| Процесс     | 333                   |
| Результат   | 52                    |

### Training Hyperparameters
- batch_size: (16, 2)
- num_epochs: (1, 16)
- max_steps: -1
- sampling_strategy: oversampling
- body_learning_rate: (2e-05, 1e-05)
- head_learning_rate: 0.01
- loss: CosineSimilarityLoss
- distance_metric: cosine_distance
- margin: 0.25
- end_to_end: False
- use_amp: False
- warmup_proportion: 0.1
- l2_weight: 0.01
- seed: 42
- eval_max_steps: -1
- load_best_model_at_end: False

### Training Results
| Epoch  | Step | Training Loss | Validation Loss |
|:------:|:----:|:-------------:|:---------------:|
| 0.0001 | 1    | 0.2551        | -               |
| 0.0070 | 50   | 0.3633        | -               |
| 0.0140 | 100  | 0.3135        | -               |
| 0.0210 | 150  | 0.2434        | -               |
| 0.0281 | 200  | 0.2361        | -               |
| 0.0351 | 250  | 0.2218        | -               |
| 0.0421 | 300  | 0.2125        | -               |
| 0.0491 | 350  | 0.2047        | -               |
| 0.0561 | 400  | 0.1975        | -               |
| 0.0631 | 450  | 0.1837        | -               |
| 0.0701 | 500  | 0.1656        | -               |
| 0.0772 | 550  | 0.1579        | -               |
| 0.0842 | 600  | 0.1444        | -               |
| 0.0912 | 650  | 0.1248        | -               |
| 0.0982 | 700  | 0.0929        | -               |
| 0.1052 | 750  | 0.085         | -               |
| 0.1122 | 800  | 0.0604        | -               |
| 0.1192 | 850  | 0.0601        | -               |
| 0.1263 | 900  | 0.0454        | -               |
| 0.1333 | 950  | 0.0489        | -               |
| 0.1403 | 1000 | 0.0375        | -               |
| 0.1473 | 1050 | 0.0318        | -               |
| 0.1543 | 1100 | 0.0314        | -               |
| 0.1613 | 1150 | 0.0281        | -               |
| 0.1684 | 1200 | 0.0283        | -               |
| 0.1754 | 1250 | 0.0248        | -               |
| 0.1824 | 1300 | 0.0206        | -               |
| 0.1894 | 1350 | 0.0139        | -               |
| 0.1964 | 1400 | 0.0201        | -               |
| 0.2034 | 1450 | 0.022         | -               |
| 0.2104 | 1500 | 0.0198        | -               |
| 0.2175 | 1550 | 0.0183        | -               |
| 0.2245 | 1600 | 0.02          | -               |
| 0.2315 | 1650 | 0.0137        | -               |
| 0.2385 | 1700 | 0.017         | -               |
| 0.2455 | 1750 | 0.0183        | -               |
| 0.2525 | 1800 | 0.0168        | -               |
| 0.2595 | 1850 | 0.0144        | -               |
| 0.2666 | 1900 | 0.0131        | -               |
| 0.2736 | 1950 | 0.0158        | -               |
| 0.2806 | 2000 | 0.0138        | -               |
| 0.2876 | 2050 | 0.0121        | -               |
| 0.2946 | 2100 | 0.0098        | -               |
| 0.3016 | 2150 | 0.0141        | -               |
| 0.3086 | 2200 | 0.0125        | -               |
| 0.3157 | 2250 | 0.0131        | -               |
| 0.3227 | 2300 | 0.0095        | -               |
| 0.3297 | 2350 | 0.0113        | -               |
| 0.3367 | 2400 | 0.0075        | -               |
| 0.3437 | 2450 | 0.0096        | -               |
| 0.3507 | 2500 | 0.0087        | -               |
| 0.3577 | 2550 | 0.0104        | -               |
| 0.3648 | 2600 | 0.0103        | -               |
| 0.3718 | 2650 | 0.0085        | -               |
| 0.3788 | 2700 | 0.009         | -               |
| 0.3858 | 2750 | 0.011         | -               |
| 0.3928 | 2800 | 0.0066        | -               |
| 0.3998 | 2850 | 0.0114        | -               |
| 0.4068 | 2900 | 0.013         | -               |
| 0.4139 | 2950 | 0.0124        | -               |
| 0.4209 | 3000 | 0.0085        | -               |
| 0.4279 | 3050 | 0.0128        | -               |
| 0.4349 | 3100 | 0.0072        | -               |
| 0.4419 | 3150 | 0.0065        | -               |
| 0.4489 | 3200 | 0.0112        | -               |
| 0.4559 | 3250 | 0.0073        | -               |
| 0.4630 | 3300 | 0.0093        | -               |
| 0.4700 | 3350 | 0.0066        | -               |
| 0.4770 | 3400 | 0.0091        | -               |
| 0.4840 | 3450 | 0.0079        | -               |
| 0.4910 | 3500 | 0.0105        | -               |
| 0.4980 | 3550 | 0.013         | -               |
| 0.5051 | 3600 | 0.0082        | -               |
| 0.5121 | 3650 | 0.0068        | -               |
| 0.5191 | 3700 | 0.0092        | -               |
| 0.5261 | 3750 | 0.0081        | -               |
| 0.5331 | 3800 | 0.0056        | -               |
| 0.5401 | 3850 | 0.005         | -               |
| 0.5471 | 3900 | 0.0111        | -               |
| 0.5542 | 3950 | 0.0067        | -               |
| 0.5612 | 4000 | 0.0078        | -               |
| 0.5682 | 4050 | 0.0078        | -               |
| 0.5752 | 4100 | 0.0054        | -               |
| 0.5822 | 4150 | 0.0071        | -               |
| 0.5892 | 4200 | 0.008         | -               |
| 0.5962 | 4250 | 0.0067        | -               |
| 0.6033 | 4300 | 0.0087        | -               |
| 0.6103 | 4350 | 0.0088        | -               |
| 0.6173 | 4400 | 0.0054        | -               |
| 0.6243 | 4450 | 0.0103        | -               |
| 0.6313 | 4500 | 0.0091        | -               |
| 0.6383 | 4550 | 0.0047        | -               |
| 0.6453 | 4600 | 0.0069        | -               |
| 0.6524 | 4650 | 0.0065        | -               |
| 0.6594 | 4700 | 0.0075        | -               |
| 0.6664 | 4750 | 0.0058        | -               |
| 0.6734 | 4800 | 0.0102        | -               |
| 0.6804 | 4850 | 0.0065        | -               |
| 0.6874 | 4900 | 0.0072        | -               |
| 0.6944 | 4950 | 0.0067        | -               |
| 0.7015 | 5000 | 0.005         | -               |
| 0.7085 | 5050 | 0.007         | -               |
| 0.7155 | 5100 | 0.0048        | -               |
| 0.7225 | 5150 | 0.0082        | -               |
| 0.7295 | 5200 | 0.0035        | -               |
| 0.7365 | 5250 | 0.0087        | -               |
| 0.7435 | 5300 | 0.0091        | -               |
| 0.7506 | 5350 | 0.0065        | -               |
| 0.7576 | 5400 | 0.0061        | -               |
| 0.7646 | 5450 | 0.0073        | -               |
| 0.7716 | 5500 | 0.005         | -               |
| 0.7786 | 5550 | 0.0042        | -               |
| 0.7856 | 5600 | 0.0039        | -               |
| 0.7926 | 5650 | 0.0076        | -               |
| 0.7997 | 5700 | 0.0083        | -               |
| 0.8067 | 5750 | 0.0079        | -               |
| 0.8137 | 5800 | 0.0073        | -               |
| 0.8207 | 5850 | 0.005         | -               |
| 0.8277 | 5900 | 0.0071        | -               |
| 0.8347 | 5950 | 0.0041        | -               |
| 0.8418 | 6000 | 0.0055        | -               |
| 0.8488 | 6050 | 0.0048        | -               |
| 0.8558 | 6100 | 0.0103        | -               |
| 0.8628 | 6150 | 0.0053        | -               |
| 0.8698 | 6200 | 0.0067        | -               |
| 0.8768 | 6250 | 0.0038        | -               |
| 0.8838 | 6300 | 0.0028        | -               |
| 0.8909 | 6350 | 0.0057        | -               |
| 0.8979 | 6400 | 0.0052        | -               |
| 0.9049 | 6450 | 0.0076        | -               |
| 0.9119 | 6500 | 0.0052        | -               |
| 0.9189 | 6550 | 0.0067        | -               |
| 0.9259 | 6600 | 0.0089        | -               |
| 0.9329 | 6650 | 0.0031        | -               |
| 0.9400 | 6700 | 0.0062        | -               |
| 0.9470 | 6750 | 0.0026        | -               |
| 0.9540 | 6800 | 0.0064        | -               |
| 0.9610 | 6850 | 0.0052        | -               |
| 0.9680 | 6900 | 0.006         | -               |
| 0.9750 | 6950 | 0.0031        | -               |
| 0.9820 | 7000 | 0.0093        | -               |
| 0.9891 | 7050 | 0.0059        | -               |
| 0.9961 | 7100 | 0.0068        | -               |

### Framework Versions
- Python: 3.11.12
- SetFit: 1.1.2
- Sentence Transformers: 4.1.0
- Transformers: 4.52.3
- PyTorch: 2.6.0+cu124
- Datasets: 3.6.0
- Tokenizers: 0.21.1

## Citation

### BibTeX
```bibtex
@article{https://doi.org/10.48550/arxiv.2209.11055,
    doi = {10.48550/ARXIV.2209.11055},
    url = {https://arxiv.org/abs/2209.11055},
    author = {Tunstall, Lewis and Reimers, Nils and Jo, Unso Eun Seo and Bates, Luke and Korat, Daniel and Wasserblat, Moshe and Pereg, Oren},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Efficient Few-Shot Learning Without Prompts},
    publisher = {arXiv},
    year = {2022},
    copyright = {Creative Commons Attribution 4.0 International}
}
```

<!--
## Glossary

*Clearly define terms in order to be accessible across audiences.*
-->

<!--
## Model Card Authors

*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->

<!--
## Model Card Contact

*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->